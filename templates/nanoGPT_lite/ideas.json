[
    {
        "Name": "layerwise_learning_rates",
        "Title": "Layer-wise Learning Rate Adaptation: Optimizing Training Dynamics in Transformer Models",
        "Experiment": "Implement layer-wise learning rates, where each transformer layer has its own learning rate. Modify the configure_optimizers function to assign different learning rates to different layers, with deeper layers having lower learning rates. Compare the training dynamics, convergence speed, and final performance with the baseline model.",
        "Interestingness": 4,
        "Feasibility": 6,
        "Novelty": 2
    },
    {
        "Name": "mix_of_experts",
        "Title": "Mix of Experts: Enhancing Transformer Efficiency Through Sparse Activation",
        "Experiment": "Modify the Block class to implement a gating mechanism that selectively activates a subset of transformer layers based on the input. This will require adding a gating function that outputs which layers to use for each input. Adjust the forward method of the GPT class to conditionally execute layers based on the gating output. Evaluate model performance and computational efficiency during and after training.",
        "Interestingness": 8,
        "Feasibility": 5,
        "Novelty": 7
    }
]